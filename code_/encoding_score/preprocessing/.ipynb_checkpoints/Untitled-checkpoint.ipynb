{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17c203c7-59d8-4204-9e18-211faaac4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_center_data(X, y):\n",
    "    \"\"\"\n",
    "    Mean centers the features (X) and target (y) data.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): The feature data.\n",
    "    y (numpy.ndarray): The target data.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray, numpy.ndarray: Mean-centered feature and target data.\n",
    "    \"\"\"\n",
    "    X_mean = X.mean(axis=0)\n",
    "    y_mean = y.mean(axis=0) if y.ndim > 1 else y.mean()\n",
    "\n",
    "    X_centered = X - X_mean\n",
    "    y_centered = y - y_mean\n",
    "\n",
    "    return X_centered, y_centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4849c7a7-274b-4183-a5a9-e15bd7cb0bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def preprocess_data(X, y, center='none', scale='none', output=None, fit_intercept=False):\n",
    "    stats = {f'{var}_{stat}': None for stat in ['mean','std','offset','scale'] for var in ['X', 'y']}\n",
    "\n",
    "    def parse_preprocessing_args(*args):\n",
    "        parsed_args = []\n",
    "        for arg in args:\n",
    "            if arg is None or len(arg) == 0:\n",
    "                parsed_args.append('none')\n",
    "            elif isinstance(arg, list):\n",
    "                parsed_args.append(''.join(arg))\n",
    "            else:\n",
    "                parsed_args.append(arg)\n",
    "        return tuple(parsed_args)\n",
    "\n",
    "    center, scale, output = parse_preprocessing_args(center, scale, output)\n",
    "    \n",
    "    if fit_intercept:\n",
    "        center += 'x'\n",
    "\n",
    "    if 'x' in center.lower():\n",
    "        stats['X_mean'] = X.mean(dim = 0)\n",
    "    if 'y' in center.lower():\n",
    "        stats['y_mean'] = y.mean(dim = 0) if y.ndim > 1 else y.mean()\n",
    "        \n",
    "    if 'x' in scale.lower():\n",
    "        stats['X_std'] = X.std(dim=0, correction=1)\n",
    "        stats['X_std'][stats['X_std'] == 0.0] = 1.0  \n",
    "    if 'y' in scale.lower():\n",
    "        stats['y_std'] = y.std(dim=0, correction=1)\n",
    "        stats['y_std'][stats['y_std'] == 0.0] = 1.0 \n",
    "    \n",
    "    if 'x' in center.lower():\n",
    "        X -= stats['X_mean']\n",
    "    if 'y' in center.lower():\n",
    "        y -= stats['y_mean']\n",
    "        \n",
    "    if 'x' in scale.lower():\n",
    "        X /= stats['X_std']\n",
    "    if 'y' in scale.lower():\n",
    "        y /= stats['y_std']\n",
    "\n",
    "    if output == 'mean_std':\n",
    "        if stats['X_mean'] is None:\n",
    "            stats['X_mean'] = X.mean(dim=0)\n",
    "        if stats['y_mean'] is None:\n",
    "            stats['y_mean'] = y.mean(dim = 0) if y.ndim > 1 else y.mean()\n",
    "        if stats['X_std'] is None:\n",
    "            stats['X_std'] = torch.ones(X.shape[1], dtype=X.dtype,  device=X.device)\n",
    "        if stats['y_std'] is None:\n",
    "            stats['y_std'] = torch.ones(y.shape[1], dtype=y.dtype,  device=y.device)\n",
    "\n",
    "    if output == 'offset_scale':\n",
    "        stats['X_offset'] = stats.pop('X_mean', None)\n",
    "        stats['y_offset'] = stats.pop('y_mean', None)\n",
    "        stats['X_scale'] = stats.pop('X_std', None)\n",
    "        if stats['X_offset'] is None:\n",
    "            stats['X_offset'] = torch.zeros(X.shape[1], dtype=X.dtype, device=X.device)\n",
    "        if stats['y_offset'] is None:\n",
    "            stats['y_offset'] = torch.zeros(y.shape[1], dtype=y.dtype, device=y.device)\n",
    "        if stats['X_scale'] is None:\n",
    "            stats['X_scale'] = torch.ones(X.shape[1], dtype=X.dtype,  device=X.device)\n",
    "\n",
    "    if output == 'offset_scale':\n",
    "        return X, y, stats['X_offset'], stats['y_offset'], stats['X_scale']\n",
    "\n",
    "    if output == 'mean_std':\n",
    "        return X, y, stats['X_mean'], stats['y_mean'], stats['X_std'], stats['y_std']\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Example usage\n",
    "# X, y = ... # your data here\n",
    "# X_preprocessed, y_preprocessed = preprocess_data(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4f1847a-7095-47fb-a659-35d982917d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import random \n",
    "from tqdm import tqdm \n",
    "import pickle \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random    \n",
    "random.seed(0)\n",
    "import scipy.stats as st\n",
    "import gc\n",
    "\n",
    "ROOT = os.getenv('BONNER_ROOT_PATH')\n",
    "sys.path.append(ROOT)\n",
    "from config import CACHE, NSD_NEURAL_DATA, NSD_SAMPLE_IMAGES    \n",
    "\n",
    "from model_evaluation.predicting_brain_data.regression.regression import regression_shared_unshared, pearson_r\n",
    "from model_evaluation.predicting_brain_data.regression.torch_cv import TorchRidgeGCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "SHARED_IDS_PATH = os.path.join(ROOT, 'image_tools','nsd_ids_shared')\n",
    "SHARED_IDS = pickle.load(open(SHARED_IDS_PATH, 'rb'))\n",
    "SHARED_IDS = [image_id.strip('.png') for image_id in SHARED_IDS]\n",
    "\n",
    "SAMPLE_IDS = pickle.load(open(NSD_SAMPLE_IMAGES, 'rb'))\n",
    "SAMPLE_IDS = [image_id.strip('.png') for image_id in SAMPLE_IDS]\n",
    "\n",
    "ALPHA_RANGE = [10**i for i in range(10)]\n",
    "    \n",
    "    \n",
    "def normalize(X):\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    X = np.nan_to_num(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "def load_nsd_data(mode: str, subject: int, region: str, return_data:bool=True) -> torch.Tensor:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Loads the neural data from disk for a particular subject and region.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mode:\n",
    "            The type of neural data to load ('shared' or 'unshared')\n",
    "            \n",
    "        subject:\n",
    "            The subject number \n",
    "        \n",
    "        region:\n",
    "            The region name\n",
    "            \n",
    "        return_ids: \n",
    "            Whether the image ids are returned \n",
    "        \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A Tensor of Neural data, or Tensor of Neural data and stimulus ids\n",
    "        \n",
    "        \"\"\"\n",
    "        path = os.path.join(NSD_NEURAL_DATA,f'roi={region}/preprocessed/z_score=session.average_across_reps=True/subject={subject}.nc')\n",
    "        \n",
    "        var_name = f'allen2021.natural_scenes.preprocessing=fithrf_GLMdenoise_RR.roi={region}.z_score=session.average_across_reps=True.subject={subject}'\n",
    "\n",
    "        \n",
    "        ds = xr.open_dataset(path, engine='h5netcdf')\n",
    "\n",
    "        if mode == 'unshared':\n",
    "            data = ds.where(~ds.presentation.stimulus_id.isin(SHARED_IDS),drop=True)\n",
    "\n",
    "        elif mode == 'shared':\n",
    "            data = ds.where(ds.presentation.stimulus_id.isin(SHARED_IDS),drop=True)\n",
    "                        \n",
    "        ids = list(data.presentation.stimulus_id.values)\n",
    "            \n",
    "        if return_data:\n",
    "            return ids, data, var_name\n",
    "        \n",
    "        else: \n",
    "            return ids\n",
    "        \n",
    "        \n",
    "            \n",
    "def filter_activations(data: xr.DataArray, ids: list) -> torch.Tensor:\n",
    "            \n",
    "        \"\"\"\n",
    "    \n",
    "        Filters model activations using image ids.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data:\n",
    "            Model activation data\n",
    "            \n",
    "        ids:\n",
    "            image ids\n",
    "        \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A Tensor of model activations filtered by image ids\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        data = data.set_index({'presentation':'stimulus_id'})\n",
    "        activations = data.sel(presentation=ids)\n",
    "        activations = activations.sortby('presentation', ascending=True)\n",
    "\n",
    "        return activations.values\n",
    "    \n",
    "    \n",
    "def normalize(X):\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    X = np.nan_to_num(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4955c294-6055-4f56-ad09-eef3eb1b228a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha: 1.0\n",
      "best score: tensor(0.4173)\n"
     ]
    }
   ],
   "source": [
    "region='V1'\n",
    "subject = 0\n",
    "device = 'cpu'\n",
    "activations_identifier= f'expansion_30_dataset=naturalscenes_subject={subject}'\n",
    "\n",
    "\n",
    "#load X_train and y_train\n",
    "X_train = xr.open_dataset(os.path.join(CACHE,'activations', activations_identifier), \n",
    "                            engine='netcdf4').x.values \n",
    "_ , neural_data_train, var_name_train = load_nsd_data(mode = 'unshared', subject = subject, region = region)\n",
    "y_train = neural_data_train[var_name_train].values\n",
    "\n",
    "X_train, y_train = mean_center_data(X_train, y_train)\n",
    "\n",
    "# corss validated ridge regression on training data to find optimal penalty term\n",
    "regression = TorchRidgeGCV(\n",
    "    alphas=ALPHA_RANGE,\n",
    "    fit_intercept=True,\n",
    "    scale_X=False,\n",
    "    scoring='pearsonr',\n",
    "    store_cv_values=False,\n",
    "    alpha_per_target=False,\n",
    "    device=device)\n",
    "\n",
    "#regression.to('cpu')\n",
    "regression.fit(X_train, y_train)\n",
    "best_alpha = float(regression.alpha_)\n",
    "print('best alpha:',best_alpha)\n",
    "print('best score:',regression.score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10f22376-a5a0-4686-9f76-9a1c0f5e7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load X_test and y_test\n",
    "X_test = xr.open_dataset(os.path.join(CACHE,'activations',f'expansion_30_dataset=naturalscenes_shared_images')).x.values#.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3cf83150-4884-4ea3-9f1b-81153fbaad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , neural_data_test, var_name_test = load_nsd_data(mode ='shared',\n",
    "                                                    subject = subject,\n",
    "                                                    region = region)           \n",
    "y_test = neural_data_test[var_name_test].values #.astype(np.float16)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0deb6d05-cd80-41aa-a7ce-7bad4252affd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:                                                                                                                (\n",
       "                                                                                                                            presentation: 872,\n",
       "                                                                                                                            neuroid: 1350)\n",
       "Coordinates:\n",
       "    x                                                                                                                      (neuroid) uint8 ...\n",
       "    y                                                                                                                      (neuroid) uint8 ...\n",
       "    z                                                                                                                      (neuroid) uint8 ...\n",
       "    stimulus_id                                                                                                            (presentation) object ...\n",
       "Dimensions without coordinates: presentation, neuroid\n",
       "Data variables:\n",
       "    allen2021.natural_scenes.preprocessing=fithrf_GLMdenoise_RR.roi=V1.z_score=session.average_across_reps=True.subject=0  (presentation, neuroid) float32 ...</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-0b4afa8a-1ca5-405f-999f-c4112d4a2547' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-0b4afa8a-1ca5-405f-999f-c4112d4a2547' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span>presentation</span>: 872</li><li><span>neuroid</span>: 1350</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-c884c80a-9668-4ac2-a18d-eff9fa3d6aea' class='xr-section-summary-in' type='checkbox'  checked><label for='section-c884c80a-9668-4ac2-a18d-eff9fa3d6aea' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>x</span></div><div class='xr-var-dims'>(neuroid)</div><div class='xr-var-dtype'>uint8</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-22223f45-7717-4b2e-a170-bd700502f471' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-22223f45-7717-4b2e-a170-bd700502f471' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ecc70dad-04b2-4b18-94ee-9219984df17c' class='xr-var-data-in' type='checkbox'><label for='data-ecc70dad-04b2-4b18-94ee-9219984df17c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1350 values with dtype=uint8]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>y</span></div><div class='xr-var-dims'>(neuroid)</div><div class='xr-var-dtype'>uint8</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-cc533441-d6c9-45ff-8a00-f8faec84e385' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-cc533441-d6c9-45ff-8a00-f8faec84e385' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-0821c17f-0553-4ea1-9ec9-274fcb9e1c39' class='xr-var-data-in' type='checkbox'><label for='data-0821c17f-0553-4ea1-9ec9-274fcb9e1c39' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1350 values with dtype=uint8]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>z</span></div><div class='xr-var-dims'>(neuroid)</div><div class='xr-var-dtype'>uint8</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-b9f81bd6-5363-4b3b-99fc-f01b9afa3d03' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-b9f81bd6-5363-4b3b-99fc-f01b9afa3d03' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-48429122-3d04-45b9-baa0-e6c74856fcd9' class='xr-var-data-in' type='checkbox'><label for='data-48429122-3d04-45b9-baa0-e6c74856fcd9' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1350 values with dtype=uint8]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>stimulus_id</span></div><div class='xr-var-dims'>(presentation)</div><div class='xr-var-dtype'>object</div><div class='xr-var-preview xr-preview'>&#x27;image02950&#x27; ... &#x27;image72948&#x27;</div><input id='attrs-65025e55-6753-4ced-b0ac-cd8d2066608c' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-65025e55-6753-4ced-b0ac-cd8d2066608c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-6ef30360-7757-4a4c-afde-74c88782506a' class='xr-var-data-in' type='checkbox'><label for='data-6ef30360-7757-4a4c-afde-74c88782506a' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;image02950&#x27;, &#x27;image02990&#x27;, &#x27;image03049&#x27;, &#x27;image03077&#x27;,\n",
       "       &#x27;image03146&#x27;, &#x27;image03157&#x27;, &#x27;image03164&#x27;, &#x27;image03171&#x27;,\n",
       "       &#x27;image03181&#x27;, &#x27;image03386&#x27;, &#x27;image03434&#x27;, &#x27;image03449&#x27;,\n",
       "       &#x27;image03489&#x27;, &#x27;image03626&#x27;, &#x27;image03682&#x27;, &#x27;image03687&#x27;,\n",
       "       &#x27;image03729&#x27;, &#x27;image03809&#x27;, &#x27;image03842&#x27;, &#x27;image03847&#x27;,\n",
       "       &#x27;image03856&#x27;, &#x27;image03913&#x27;, &#x27;image03951&#x27;, &#x27;image04051&#x27;,\n",
       "       &#x27;image04058&#x27;, &#x27;image04129&#x27;, &#x27;image04156&#x27;, &#x27;image04249&#x27;,\n",
       "       &#x27;image04423&#x27;, &#x27;image04436&#x27;, &#x27;image04667&#x27;, &#x27;image04690&#x27;,\n",
       "       &#x27;image04768&#x27;, &#x27;image04786&#x27;, &#x27;image04835&#x27;, &#x27;image04869&#x27;,\n",
       "       &#x27;image04892&#x27;, &#x27;image04930&#x27;, &#x27;image05034&#x27;, &#x27;image05106&#x27;,\n",
       "       &#x27;image05204&#x27;, &#x27;image05301&#x27;, &#x27;image05338&#x27;, &#x27;image05459&#x27;,\n",
       "       &#x27;image05542&#x27;, &#x27;image05583&#x27;, &#x27;image05602&#x27;, &#x27;image05714&#x27;,\n",
       "       &#x27;image05878&#x27;, &#x27;image06199&#x27;, &#x27;image06222&#x27;, &#x27;image06431&#x27;,\n",
       "       &#x27;image06444&#x27;, &#x27;image06489&#x27;, &#x27;image06514&#x27;, &#x27;image06521&#x27;,\n",
       "       &#x27;image06558&#x27;, &#x27;image06713&#x27;, &#x27;image06801&#x27;, &#x27;image07007&#x27;,\n",
       "       &#x27;image07039&#x27;, &#x27;image07120&#x27;, &#x27;image07207&#x27;, &#x27;image07336&#x27;,\n",
       "       &#x27;image07366&#x27;, &#x27;image07409&#x27;, &#x27;image07418&#x27;, &#x27;image07480&#x27;,\n",
       "       &#x27;image07654&#x27;, &#x27;image07659&#x27;, &#x27;image07840&#x27;, &#x27;image07859&#x27;,\n",
       "       &#x27;image07944&#x27;, &#x27;image07948&#x27;, &#x27;image07954&#x27;, &#x27;image08006&#x27;,\n",
       "       &#x27;image08109&#x27;, &#x27;image08204&#x27;, &#x27;image08225&#x27;, &#x27;image08262&#x27;,\n",
       "...\n",
       "       &#x27;image66216&#x27;, &#x27;image66278&#x27;, &#x27;image66330&#x27;, &#x27;image66342&#x27;,\n",
       "       &#x27;image66421&#x27;, &#x27;image66464&#x27;, &#x27;image66479&#x27;, &#x27;image66489&#x27;,\n",
       "       &#x27;image66580&#x27;, &#x27;image66773&#x27;, &#x27;image66836&#x27;, &#x27;image66946&#x27;,\n",
       "       &#x27;image66976&#x27;, &#x27;image67045&#x27;, &#x27;image67113&#x27;, &#x27;image67204&#x27;,\n",
       "       &#x27;image67237&#x27;, &#x27;image67252&#x27;, &#x27;image67295&#x27;, &#x27;image67363&#x27;,\n",
       "       &#x27;image67742&#x27;, &#x27;image67802&#x27;, &#x27;image67829&#x27;, &#x27;image68023&#x27;,\n",
       "       &#x27;image68168&#x27;, &#x27;image68278&#x27;, &#x27;image68339&#x27;, &#x27;image68418&#x27;,\n",
       "       &#x27;image68471&#x27;, &#x27;image68741&#x27;, &#x27;image68814&#x27;, &#x27;image68842&#x27;,\n",
       "       &#x27;image68858&#x27;, &#x27;image68897&#x27;, &#x27;image69007&#x27;, &#x27;image69130&#x27;,\n",
       "       &#x27;image69214&#x27;, &#x27;image69240&#x27;, &#x27;image69502&#x27;, &#x27;image69614&#x27;,\n",
       "       &#x27;image69785&#x27;, &#x27;image69831&#x27;, &#x27;image69839&#x27;, &#x27;image69854&#x27;,\n",
       "       &#x27;image69918&#x27;, &#x27;image70038&#x27;, &#x27;image70075&#x27;, &#x27;image70095&#x27;,\n",
       "       &#x27;image70193&#x27;, &#x27;image70232&#x27;, &#x27;image70335&#x27;, &#x27;image70360&#x27;,\n",
       "       &#x27;image70427&#x27;, &#x27;image70505&#x27;, &#x27;image70589&#x27;, &#x27;image70758&#x27;,\n",
       "       &#x27;image70764&#x27;, &#x27;image71186&#x27;, &#x27;image71229&#x27;, &#x27;image71232&#x27;,\n",
       "       &#x27;image71241&#x27;, &#x27;image71410&#x27;, &#x27;image71450&#x27;, &#x27;image71753&#x27;,\n",
       "       &#x27;image71894&#x27;, &#x27;image71928&#x27;, &#x27;image72015&#x27;, &#x27;image72080&#x27;,\n",
       "       &#x27;image72170&#x27;, &#x27;image72209&#x27;, &#x27;image72257&#x27;, &#x27;image72312&#x27;,\n",
       "       &#x27;image72510&#x27;, &#x27;image72605&#x27;, &#x27;image72719&#x27;, &#x27;image72948&#x27;],\n",
       "      dtype=object)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-f86c4ecf-23a6-4f9e-b8f4-99b7bd68c214' class='xr-section-summary-in' type='checkbox'  checked><label for='section-f86c4ecf-23a6-4f9e-b8f4-99b7bd68c214' class='xr-section-summary' >Data variables: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>allen2021.natural_scenes.preprocessing=fithrf_GLMdenoise_RR.roi=V1.z_score=session.average_across_reps=True.subject=0</span></div><div class='xr-var-dims'>(presentation, neuroid)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>-0.09324 0.6798 ... -0.3728 -0.2147</div><input id='attrs-254e226c-72ab-4bfb-8c45-af568848be4b' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-254e226c-72ab-4bfb-8c45-af568848be4b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-68e7ecc6-cf63-43cd-bbf6-2c2f5dc34034' class='xr-var-data-in' type='checkbox'><label for='data-68e7ecc6-cf63-43cd-bbf6-2c2f5dc34034' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([[-0.0932436 ,  0.6798253 ,  0.839222  , ...,  0.13020866,\n",
       "         1.2909118 , -0.45062009],\n",
       "       [-0.36064884, -0.85859495, -0.44466862, ..., -0.47644636,\n",
       "        -0.13295712,  0.4860421 ],\n",
       "       [ 0.01267737, -0.6862054 , -0.7140854 , ...,  0.9549141 ,\n",
       "        -0.12952662,  0.21679282],\n",
       "       ...,\n",
       "       [-0.44278845,  0.40842894, -0.42881608, ..., -0.7761671 ,\n",
       "         0.7875963 ,  0.48590913],\n",
       "       [-0.22477429, -0.83322114,  0.8235683 , ...,  0.5175818 ,\n",
       "        -0.5441716 ,  0.09911594],\n",
       "       [-0.00382607,  0.36365023, -0.26071012, ...,  0.13752837,\n",
       "        -0.37277094, -0.21473086]], dtype=float32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-b6e75ad2-8583-4372-9fe9-6652bc094b78' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-b6e75ad2-8583-4372-9fe9-6652bc094b78' class='xr-section-summary'  title='Expand/collapse section'>Indexes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-b341edad-2586-4fbb-8da1-a30349225554' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-b341edad-2586-4fbb-8da1-a30349225554' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:                                                                                                                (\n",
       "                                                                                                                            presentation: 872,\n",
       "                                                                                                                            neuroid: 1350)\n",
       "Coordinates:\n",
       "    x                                                                                                                      (neuroid) uint8 ...\n",
       "    y                                                                                                                      (neuroid) uint8 ...\n",
       "    z                                                                                                                      (neuroid) uint8 ...\n",
       "    stimulus_id                                                                                                            (presentation) object ...\n",
       "Dimensions without coordinates: presentation, neuroid\n",
       "Data variables:\n",
       "    allen2021.natural_scenes.preprocessing=fithrf_GLMdenoise_RR.roi=V1.z_score=session.average_across_reps=True.subject=0  (presentation, neuroid) float32 ..."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76a5a4af-4aba-4d6b-b118-e7211eecad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing_kwargs = {'output': 'offset_scale'}\n",
    "# preprocessing_kwargs['center'] = 'x'\n",
    "# X_train, y_train, X_offset, y_offset, X_scale = preprocess_data(torch.Tensor(X_train), torch.Tensor(y_train), **preprocessing_kwargs)\n",
    "# X_test, y_test, X_offset, y_offset, X_scale = preprocess_data(torch.Tensor(X_test), torch.Tensor(y_test), **preprocessing_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "664a94b7-66b9-414e-a624-cae1a454fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def min_max_scale(X, feature_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    Scales the features of X to a specified range.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): The data to be scaled.\n",
    "    feature_range (tuple): The desired range of transformed data.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Scaled data.\n",
    "    \"\"\"\n",
    "    X_min, X_max = X.min(axis=0), X.max(axis=0)\n",
    "    scale = feature_range[1] - feature_range[0]\n",
    "    min_range = feature_range[0]\n",
    "\n",
    "    # Avoid division by zero\n",
    "    X_std = (X - X_min) / (X_max - X_min)\n",
    "    X_scaled = X_std * scale + min_range\n",
    "\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0461bec2-e513-404b-a8dc-4616049917dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = min_max_scale(X_test)\n",
    "y_test = min_max_scale(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a336dd9-dfc4-4a03-899f-fe240aa38b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, y_train = mean_center_data(X_train, y_train)\n",
    "#X_test, y_test = mean_center_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "964c9e83-5dcf-47b0-bebc-c2f82c96230a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model= Ridge(alpha=1000000)\n",
    "#model.fit(X_train, y_train)\n",
    "y_predicted = regression.predict(X_test)\n",
    "r = pearson_r(torch.Tensor(y_test),torch.Tensor(y_predicted))\n",
    "r.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0fd2f32-62f2-41a1-81c8-6b1ef0793972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b56680c-5438-4fd2-b8fb-3c61148dd606",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.append(r.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9cdd2-2f2b-43ae-88ec-78fdc1354bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
